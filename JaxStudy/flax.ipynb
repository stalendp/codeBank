{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flax Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A simple example\n",
    "\n",
    "refer:  https://github.com/google/flax/tree/main/examples/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def get_datasets():\n",
    "  \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n",
    "  ds_builder = tfds.builder('mnist')\n",
    "  ds_builder.download_and_prepare()\n",
    "  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "  train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
    "  test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n",
    "  return train_ds, test_ds\n",
    "\n",
    "# train_ds, test_ds = get_datasets()\n",
    "# train_ds_size = len(train_ds['image'])\n",
    "# steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "# key, input_rng = jax.random.split(key)\n",
    "# perms = jax.random.permutation(input_rng, len(train_ds['image']))\n",
    "# perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "# perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "## one Epoch\n",
    "# batch_images = train_ds['image'][perm, ...]\n",
    "# batch_labels = train_ds['label'][perm, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = nn.Dense(features=256)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   2. invoke example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "def create_train_state(rng, config):\n",
    "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "  cnn = CNN()\n",
    "  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "  tx = optax.sgd(config.learning_rate, config.momentum)\n",
    "  return train_state.TrainState.create(\n",
    "      apply_fn=cnn.apply, params=params, tx=tx) # 包含CNN参数 和 优化参数\n",
    "\n",
    "# rng, init_rng = jax.random.split(rng)\n",
    "# state = create_train_state(init_rng, config) \n",
    "\n",
    "@jax.jit\n",
    "def apply_model(state, images, labels):\n",
    "  \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "  def loss_fn(params):\n",
    "    logits = state.apply_fn({'params': params}, images) # <- CNN计算\n",
    "    one_hot = jax.nn.one_hot(labels, 10)\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "    return loss, logits\n",
    "\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True) # add grad calculation\n",
    "  (loss, logits), grads = grad_fn(state.params)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  return grads, loss, accuracy\n",
    "\n",
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "  return state.apply_gradients(grads=grads)  # apply grads to model state\n",
    "\n",
    "# grads, loss, accuracy = apply_model(state, batch_images, batch_labels)\n",
    "# state = update_model(state, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "batch_size: 128\n",
       "learning_rate: 0.1\n",
       "momentum: 0.9\n",
       "num_epochs: 10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from myconfigs import default as config_lib\n",
    "config = config_lib.get_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Logger & tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:helloworld!\n"
     ]
    }
   ],
   "source": [
    "from absl import logging\n",
    "from flax.metrics import tensorboard\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "logging.info(\"helloworld!\")\n",
    "# logging.info(\n",
    "#     'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'\n",
    "#     % (epoch, train_loss, train_accuracy * 100, test_loss,\n",
    "#         test_accuracy * 100))\n",
    "workdir = \"./logger\"\n",
    "summary_writer = tensorboard.SummaryWriter(workdir)\n",
    "summary_writer.hparams(dict(config))\n",
    "\n",
    "# epoch = 1\n",
    "# train_loss = 10.1\n",
    "# summary_writer.scalar('train_loss', train_loss, epoch)\n",
    "\n",
    "# summary_writer.scalar('train_loss', train_loss, epoch)\n",
    "# summary_writer.scalar('train_accuracy', train_accuracy, epoch)\n",
    "# summary_writer.scalar('test_loss', test_loss, epoch)\n",
    "# summary_writer.scalar('test_accuracy', test_accuracy, epoch)\n",
    "\n",
    "# summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config: ml_collections.ConfigDict,\n",
    "                       workdir: str) -> train_state.TrainState:\n",
    "  \"\"\"Execute model training and evaluation loop.\n",
    "\n",
    "  Args:\n",
    "    config: Hyperparameter configuration for training and evaluation.\n",
    "    workdir: Directory where the tensorboard summaries are written to.\n",
    "\n",
    "  Returns:\n",
    "    The train state (which includes the `.params`).\n",
    "  \"\"\"\n",
    "  train_ds, test_ds = get_datasets()\n",
    "  rng = jax.random.PRNGKey(0)\n",
    "\n",
    "  summary_writer = tensorboard.SummaryWriter(workdir)\n",
    "  summary_writer.hparams(dict(config))\n",
    "\n",
    "  rng, init_rng = jax.random.split(rng)\n",
    "  state = create_train_state(init_rng, config)\n",
    "\n",
    "  for epoch in range(1, config.num_epochs + 1):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    state, train_loss, train_accuracy = train_epoch(state, train_ds,\n",
    "                                                    config.batch_size,\n",
    "                                                    input_rng)\n",
    "    _, test_loss, test_accuracy = apply_model(state, test_ds['image'],\n",
    "                                              test_ds['label'])\n",
    "\n",
    "    logging.info(\n",
    "        'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'\n",
    "        % (epoch, train_loss, train_accuracy * 100, test_loss,\n",
    "           test_accuracy * 100))\n",
    "\n",
    "    summary_writer.scalar('train_loss', train_loss, epoch)\n",
    "    summary_writer.scalar('train_accuracy', train_accuracy, epoch)\n",
    "    summary_writer.scalar('test_loss', test_loss, epoch)\n",
    "    summary_writer.scalar('test_accuracy', test_accuracy, epoch)\n",
    "\n",
    "  summary_writer.flush()\n",
    "  return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from ~/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.supervised_keys from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (~/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split train, from ~/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from ~/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Remote TPU is not linked into jax; skipping remote TPU.\n",
      "INFO:absl:Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'\n",
      "INFO:absl:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:absl:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:absl:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     show_img(img, axs[i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n][i \u001b[38;5;241m%\u001b[39m n], title)\n\u001b[1;32m     21\u001b[0m train_ds, test_ds \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mget_datasets()\n\u001b[0;32m---> 22\u001b[0m \u001b[43mshow_img_grid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrain_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [25], line 16\u001b[0m, in \u001b[0;36mshow_img_grid\u001b[0;34m(imgs, titles)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_img_grid\u001b[39m(imgs, titles):\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;124;03m\"\"\"Shows a grid of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m   n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(imgs)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m.5\u001b[39m))\n\u001b[1;32m     17\u001b[0m   _, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(n, n, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m n, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m n))\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i, (img, title) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(imgs, titles)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import train\n",
    "\n",
    "def show_img(img, ax=None, title=None):\n",
    "  \"\"\"Shows a single image.\"\"\"\n",
    "  if ax is None:\n",
    "    ax = plt.gca()\n",
    "  ax.imshow(img[..., 0], cmap='gray')\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  if title:\n",
    "    ax.set_title(title)\n",
    "\n",
    "def show_img_grid(imgs, titles):\n",
    "  \"\"\"Shows a grid of images.\"\"\"\n",
    "  n = int(np.ceil(len(imgs)**.5))\n",
    "  _, axs = plt.subplots(n, n, figsize=(3 * n, 3 * n))\n",
    "  for i, (img, title) in enumerate(zip(imgs, titles)):\n",
    "    show_img(img, axs[i // n][i % n], title)\n",
    "\n",
    "train_ds, test_ds = train.get_datasets()\n",
    "show_img_grid(\n",
    "    [train_ds['image'][idx] for idx in range(25)],\n",
    "    [f'label={train_ds[\"label\"][idx]}' for idx in range(25)],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8795502de6d0a7d7bc9ca79773498af11dd04849ae557535fdc41c5621c59687"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
